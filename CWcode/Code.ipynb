{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imblearn\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "#import data\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()\n",
    "\n",
    "questions = pd.read_csv('questions.csv')\n",
    "questions.describe()\n",
    "\n",
    "questions.head()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data0 = pd.read_csv(\"neededcode.csv\")\n",
    "\n",
    "data0.describe()\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "ohe.fit_transform(data0[['gender']])\n",
    "\n",
    "ohe.categories_\n",
    "\n",
    "#checking for missing values\n",
    "x = data0.isnull().sum()\n",
    "set(x)\n",
    "\n",
    "#Removing data that has zero accuracy\n",
    "data0 = data0[data.accuracy != 0]\n",
    "\n",
    "modified_values = data[data.age<120].age\n",
    "print(modified_values.mean())\n",
    "print(modified_values.median())\n",
    "print(modified_values.mode()[0])\n",
    "\n",
    "age_outliers = []\n",
    "\n",
    "for value in data0.age.unique():\n",
    "    if value > 120: \n",
    "        age_outliers.append(value)\n",
    "#These are clearly outliers since the oldest human know lived to 120; this is clearly an anomoly\n",
    "#currently replacing with median of age column \n",
    "age_median = data0.age.median()\n",
    "data0.age = np.where(data0.age > 120, age_median, data0.age)\n",
    "\n",
    "data0.age.describe()\n",
    "\n",
    "gender_mode = data0.gender.mode() #gender can only be 1,2,3 not 0\n",
    "data0.gender = np.where(data0.gender == 0, gender_mode, data0.gender)\n",
    "\n",
    "#replace the rows where affiliative score is 5.1\n",
    "data0 = data0[data.affiliative <5.1]\n",
    "\n",
    "data0.describe()\n",
    "\n",
    "#male = np.where(data0.gender == 1)\n",
    "male = data0[data0.gender == 1].value_counts()\n",
    "print(male)\n",
    "female = data0[data0.gender == 2].value_counts()\n",
    "print(female)\n",
    "others = data0[data0.gender == 3].value_counts()\n",
    "print(others)\n",
    "\n",
    "from collections import Counter\n",
    "import collections\n",
    "indexs_with_minus = []\n",
    "\n",
    "data[\"mod_ac\"] = pd.cut(data.accuracy, bins = [0,10,20,30,40,50,60,70,80,90,100])\n",
    "\n",
    "for value in data:\n",
    "    a = list(data[data[value] == float(-1)].index)\n",
    "    indexs_with_minus.extend(a)\n",
    "\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for value in set(indexs_with_minus):\n",
    "      accuracy_list.append(data.iloc[value].mod_ac)\n",
    "Counter(accuracy_list)\n",
    "\n",
    "\n",
    "plt.title('Age')\n",
    "data0.age.hist()\n",
    "\n",
    "plt.title('Affiliative Score')\n",
    "data0.affiliative.hist()\n",
    "\n",
    "plt.title('Selfenhancing Score')\n",
    "data0.selfenhancing.hist()\n",
    "\n",
    "plt.title('Aggressive Score')\n",
    "data0.agressive.hist()\n",
    "\n",
    "plt.title('Selfdefeating Score')\n",
    "data0.selfdefeating.hist()\n",
    "\n",
    "plt.title('Gender')\n",
    "data0.gender.hist()\n",
    "\n",
    "plt.title('Accuracy')\n",
    "data0.accuracy.hist()\n",
    "\n",
    "plt.title('Accuracy')\n",
    "sns.violinplot(data = data0.accuracy,size=4)\n",
    "plt.grid() \n",
    "\n",
    "plt.title('Age')\n",
    "sns.violinplot(data = data0.age,size=4)\n",
    "plt.grid() \n",
    "\n",
    "plt.title('Affiliative Score')\n",
    "sns.violinplot(data = data0.agressive,size=4)\n",
    "plt.grid() \n",
    "\n",
    "plt.title('Agressive Score')\n",
    "sns.violinplot(data = data0.agressive,size=4)\n",
    "plt.grid()\n",
    "\n",
    "plt.title('Selfdefeating Score')\n",
    "sns.violinplot(data = data0.selfdefeating,size=4)\n",
    "plt.grid()\n",
    "\n",
    "plt.title('Selfenhancing Score')\n",
    "sns.violinplot(data = data0.selfenhancing,size=4)\n",
    "plt.grid()\n",
    "\n",
    "plt.title('Gender')\n",
    "sns.violinplot(data = data0.gender,size=4)\n",
    "plt.grid() \n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.pairplot(data0,hue = \"gender\", height = 3)\n",
    "\n",
    "corrMatrix = data0.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "#remove outliers for affiliative score\n",
    "Q1_1 = data.affiliative.quantile(0.25)\n",
    "Q3_1 = data.affiliative.quantile(0.75)\n",
    "IQR_1 = Q3_1 - Q1_1\n",
    "lower_limit_1 = Q1_1 - 1.5*IQR_1\n",
    "upper_limit_1 = Q3_1 + 1.5*IQR_1\n",
    "print(lower_limit_1,upper_limit_1)\n",
    "#data0[(data0.affiliative<lower_limit_1)|data0.affiliative>upper_limit_1]\n",
    "data0 = data0[(data0.affiliative>lower_limit_1)&(data0.affiliative<upper_limit_1)]\n",
    "#data0.describe()\n",
    "data0.describe()\n",
    "\n",
    "#remove outliers for selfenhancing score\n",
    "Q1_2 = data.selfenhancing.quantile(0.25)\n",
    "Q3_2 = data.selfenhancing.quantile(0.75)\n",
    "IQR_2 = Q3_2 - Q1_2\n",
    "lower_limit_2 = Q1_2 - 1.5*IQR_2\n",
    "upper_limit_2 = Q3_2 + 1.5*IQR_2\n",
    "print(lower_limit_2,upper_limit_2)\n",
    "data0 = data0[(data0.selfenhancing>lower_limit_2)&(data0.selfenhancing<upper_limit_2)]\n",
    "data0.describe()\n",
    "#data_no_outlier = data0[(data0.affiliative>lower_limit_1)&data0.affiliative<upper_limit_1]\n",
    "#data_no_outlier.describe()\n",
    "\n",
    "#remove outliers for agressive score\n",
    "Q1_3 = data.agressive.quantile(0.25)\n",
    "Q3_3 = data.agressive.quantile(0.75)\n",
    "IQR_3 = Q3_3 - Q1_3\n",
    "lower_limit_3 = Q1_3 - 1.5*IQR_3\n",
    "upper_limit_3 = Q3_3 + 1.5*IQR_3\n",
    "print(lower_limit_3,upper_limit_3)\n",
    "data0 = data0[(data0.agressive>lower_limit_3)&(data0.agressive<upper_limit_3)]\n",
    "data0.describe()\n",
    "#data_no_outlier = data0[(data0.affiliative>lower_limit_1)&data0.affiliative<upper_limit_1]\n",
    "#data_no_outlier.describe()\n",
    "\n",
    "#remove outliers for agressive score\n",
    "Q1_4 = data.selfdefeating.quantile(0.25)\n",
    "Q3_4 = data.selfdefeating.quantile(0.75)\n",
    "IQR_4 = Q3_4 - Q1_4\n",
    "lower_limit_4 = Q1_4 - 1.5*IQR_4\n",
    "upper_limit_4 = Q3_4 + 1.5*IQR_4\n",
    "print(lower_limit_4,upper_limit_4)\n",
    "data0 = data0[(data0.selfdefeating>lower_limit_4)&(data0.selfdefeating<upper_limit_4)]\n",
    "data0.describe()\n",
    "#data_no_outlier = data0[(data0.affiliative>lower_limit_1)&data0.affiliative<upper_limit_1]\n",
    "#data_no_outlier.describe()\n",
    "\n",
    "# Create X (all the feature columns)\n",
    "#X = data.drop([\"gender\"],axis=1)\n",
    "# Create y (the target column)\n",
    "#y = data[[\"gender\"]]\n",
    "#data.columns\n",
    "X = data0[['affiliative', 'selfenhancing', 'agressive', 'selfdefeating', 'age','accuracy']]\n",
    "Y = data0['gender']\n",
    "\n",
    "X\n",
    "\n",
    "Y\n",
    "#y = label_binarize(y, classes=[0, 1, 2])\n",
    "#n_classes = y.shape[1]\n",
    "\n",
    "\n",
    "#split into train and test values, using minmax scaled data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size = 0.2)\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n",
    "\n",
    "#Apply SMOTE to balance gender\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "Xtrain_smote, Ytrain_smote = oversample.fit_resample(X_train.astype('int'),Y_train)\n",
    "from collections import Counter\n",
    "print(\"Y Before SMOTE:\", Counter(Y_train))\n",
    "print(\"Y After SMOTE:\", Counter(Ytrain_smote))\n",
    "\n",
    "Xtrain_smote.shape, Ytrain_smote.shape\n",
    "\n",
    "# StandardScalar\n",
    "StandardScaler = preprocessing.StandardScaler()\n",
    "Standard_scaled = StandardScaler.fit_transform(X)\n",
    "Standard_scaled_data = pd.DataFrame(Standard_scaled,columns = X.columns.values,index=X.index)\n",
    "Standard_scaled_data\n",
    "\n",
    "#MinMaxScaler\n",
    "MinMaxScaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "MinMax_scaled = MinMaxScaler.fit_transform(X)\n",
    "MinMax_scaled_data = pd.DataFrame(MinMax_scaled,columns = X.columns.values,index = X.index)\n",
    "MinMax_scaled_data\n",
    "\n",
    "#Robust Scaler\n",
    "Robust_Scaler = preprocessing.RobustScaler()\n",
    "Robust_Scaled = Robust_Scaler.fit_transform(X)\n",
    "Robust_Scaled_data = pd.DataFrame(Robust_Scaled,columns = X.columns.values,index = X.index)\n",
    "Robust_Scaled_data\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4,figsize=(30,10))\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_smote['affiliative'], ax=ax1)\n",
    "sns.kdeplot(X_smote['selfenhancing'], ax=ax1)\n",
    "sns.kdeplot(X_smote['agressive'], ax=ax1)\n",
    "sns.kdeplot(X_smote['selfdefeating'], ax=ax1)\n",
    "sns.kdeplot(X_smote['age'], ax=ax1)\n",
    "sns.kdeplot(X_smote['accuracy'], ax=ax1)\n",
    "\n",
    "\n",
    "ax2.set_title('After Standard Scaler')\n",
    "sns.kdeplot(Standard_scaled_data['affiliative'], ax=ax2)\n",
    "sns.kdeplot(Standard_scaled_data['selfenhancing'], ax=ax2)\n",
    "sns.kdeplot(Standard_scaled_data['agressive'], ax=ax2)\n",
    "sns.kdeplot(Standard_scaled_data['selfdefeating'], ax=ax2)\n",
    "sns.kdeplot(Standard_scaled_data['age'], ax=ax2)\n",
    "sns.kdeplot(Standard_scaled_data['accuracy'], ax=ax2)\n",
    "\n",
    "ax3.set_title('After Normalization/MinMax Scaler')\n",
    "sns.kdeplot(MinMax_scaled_data['affiliative'], ax=ax3)\n",
    "sns.kdeplot(MinMax_scaled_data['selfenhancing'], ax=ax3)\n",
    "sns.kdeplot(MinMax_scaled_data['agressive'], ax=ax3)\n",
    "sns.kdeplot(MinMax_scaled_data['selfdefeating'], ax=ax3)\n",
    "sns.kdeplot(MinMax_scaled_data['age'], ax=ax3)\n",
    "sns.kdeplot(MinMax_scaled_data['accuracy'], ax=ax3)\n",
    "\n",
    "ax4.set_title('After Robust Scaler')\n",
    "sns.kdeplot(Robust_Scaled_data['affiliative'], ax=ax4)\n",
    "sns.kdeplot(Robust_Scaled_data['selfenhancing'], ax=ax4)\n",
    "sns.kdeplot(Robust_Scaled_data['agressive'], ax=ax4)\n",
    "sns.kdeplot(Robust_Scaled_data['selfdefeating'], ax=ax4)\n",
    "sns.kdeplot(Robust_Scaled_data['age'], ax=ax4)\n",
    "sns.kdeplot(Robust_Scaled_data['accuracy'], ax=ax4)\n",
    "plt.show()\n",
    "\n",
    "#apply min-max scaler to scale the train and test data\n",
    "MinMaxScaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "MinMax_scaled_X_train = MinMaxScaler.fit_transform(Xtrain_smote)\n",
    "#Robust_Scaled_data = pd.DataFrame(Robust_Scaled,columns = X.columns.values,index = X.index)\n",
    "#MinMax_scaled_data = pd.DataFrame(MinMax_scaled_X_train,columns = MinMax_scaled_X_train.columns,index = MinMax_scaled_X_train)\n",
    "#MinMax_scaled_data\n",
    "MinMax_scaled_X_test = MinMaxScaler.fit_transform(X_test)\n",
    "\n",
    "MinMax_scaled_X_train.shape\n",
    "\n",
    "#################################################\n",
    "#First we test on standard situation, where we care all three class equally or expecting the third class to have a higher accuracy\n",
    "#we have up sampled and scale on X training set, and upsampled on Y traning set, and scaled on the X test set. \n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## Define a evlauation function for hyperparameter tuning \n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "#predict using random forest classifier\n",
    "#standard method without using any corss validation or hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Instantiate and fit the model (on the training set)\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "# Check the score of the model (on both training and test set)\n",
    "print(f\"Random Forest Classifier Accuracy on training set: {RFC.score(MinMax_scaled_X_train, Ytrain_smote) * 100:.2f}%\")\n",
    "print(f\"Random Forest Classifier Accuracy: {RFC.score(MinMax_scaled_X_test, Y_test) * 100:.2f}%\")\n",
    "\n",
    "###Check the default parameters\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(RFC.get_params())\n",
    "\n",
    "########## apply random hyperparameter search to \n",
    "# find a rough range of best parameters\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "RFC_test = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "RFC_test_random = RandomizedSearchCV(estimator = RFC_test, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "RFC_test_random.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "\n",
    "RFC_test_random.best_params_\n",
    "\n",
    "##Evaluate the result of randomsearchcv\n",
    "\n",
    "best_random = RFC_test_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, MinMax_scaled_X_test, Y_test)\n",
    "\n",
    "#apply cross validation with 10 folds and Grid Search for hyperparameters\n",
    "#grid selection based on random search cv\n",
    "param_grid = {\n",
    "    'bootstrap': [True,False],\n",
    "    'max_depth': [10, 30, 60, 90],\n",
    "    'max_features': [2, 3, 'sqrt'],\n",
    "    'min_samples_leaf': [1 , 3],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 500, 1000, 1800]\n",
    "}\n",
    "\n",
    "RFC_tuned = RandomForestClassifier()\n",
    "grid_search_RFC = GridSearchCV(estimator = RFC_tuned,param_grid=param_grid,\n",
    "                           cv = 10, n_jobs = -1, verbose = 2)\n",
    "\n",
    "\n",
    "\n",
    "grid_search_RFC.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "grid_search_RFC.best_params_\n",
    "\n",
    "\n",
    "################## Check the best hyperparameter score\n",
    "\n",
    "best_grid_RFC = grid_search_RFC.best_estimator_\n",
    "grid_training_accuracy_RFC = evaluate(best_grid_RFC, MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "grid_accuracy_RFC = evaluate(best_grid_RFC, MinMax_scaled_X_test, Y_test)\n",
    "grid_search_RFC.best_score_\n",
    "\n",
    "############ Plain decision trees without any hyperparameters or\n",
    "# cross validatoin\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Instantiate and fit the model (on the training set)\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "# Check the score of the model (on both training and test set)\n",
    "print(f\"Decision Tree Accuracy on training set: {clf.score(MinMax_scaled_X_train, Ytrain_smote) * 100:.2f}%\")\n",
    "print(f\"Decision Tree Accuracy: {clf.score(MinMax_scaled_X_test, Y_test) * 100:.2f}%\")\n",
    "\n",
    "###Check the default parameters\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(DT.get_params())\n",
    "\n",
    "### First apply random grid search to find a rough range\n",
    "#class weight not assigned since we already upsampled\n",
    "DT_random_grid = {'criterion':['gini','entropy'],\n",
    "             'max_depth':[4, 5, 6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "             'splitter':['best','random'],\n",
    "             #'min_samples_split':[1,2,'none'],\n",
    "             'min_samples_leaf':[1,2,3,4,5,8,10,20,30],\n",
    "             #'min_weight_fraction_leaf':[0,1,2,3],     \n",
    "             #'max_features':['none','auto','log2'],\n",
    "             'min_impurity_decrease':[0,1,2,3]\n",
    "    \n",
    "}\n",
    "pprint(DT_random_grid)\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "DT_test = DecisionTreeClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation\n",
    "\n",
    "DT_test_random = RandomizedSearchCV(estimator = DT_test, param_distributions = DT_random_grid, cv = 3, random_state=42)\n",
    "# Fit the random search model\n",
    "DT_test_random.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "\n",
    "DT_test_random.best_params_\n",
    "\n",
    "##Evaluate the result of randomsearchcv\n",
    "\n",
    "DT_best_random = DT_test_random.best_estimator_\n",
    "DT_random_accuracy = evaluate(DT_best_random, MinMax_scaled_X_test, Y_test)\n",
    "\n",
    "#### Method with cross validaiton and GRID SEARCH\n",
    "tree_para = {'criterion':['gini','entropy'],\n",
    "             'max_depth':[1,10,15,20,30,40,55,70,100,120],\n",
    "             'splitter': ['random'],\n",
    "             'min_impurity_decrease': [0],\n",
    "             'min_samples_leaf': [1, 3, 5, 10, 20]\n",
    "            }\n",
    "\n",
    "DT_tuned = DecisionTreeClassifier()\n",
    "grid_search_DT = GridSearchCV(estimator = DT_tuned,param_grid=tree_para,cv = 10)\n",
    "\n",
    "grid_search_DT.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "grid_search_DT.best_params_\n",
    "\n",
    "\n",
    "################## Check the best hyperparameter score\n",
    "\n",
    "best_grid_DT = grid_search_DT.best_estimator_\n",
    "grid_training_accuracy_DT = evaluate(best_grid_DT, MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "grid_accuracy_DT = evaluate(best_grid_DT, MinMax_scaled_X_test, Y_test)\n",
    "grid_search_DT.best_score_\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors= 5)\n",
    "neigh.fit(MinMax_scaled_X_train, Ytrain_smote.values.ravel())\n",
    "print(f\"K nearest Neightbour Classifier Accuracy on training set: {neigh.score(MinMax_scaled_X_train, Ytrain_smote) * 100:.2f}%\")\n",
    "print(f\"K nearest Neightbour Classifier Accuracy: {neigh.score(MinMax_scaled_X_test, Y_test) * 100:.2f}%\")\n",
    "\n",
    "###Check the default parameters\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(neigh.get_params())\n",
    "\n",
    "##Using randomized grid search\n",
    "\n",
    "\n",
    "\n",
    "###############Using other evaluation metrices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Confusion matrix\n",
    "plot_confusion_matrix(RFC, MinMax_scaled_X_test, Y_test)\n",
    "\n",
    "#for Decision Trees\n",
    "plot_confusion_matrix(DT, MinMax_scaled_X_test, Y_test)\n",
    "\n",
    "#for K Nearest Neighbours\n",
    "plot_confusion_matrix(neigh, MinMax_scaled_X_test, Y_test)\n",
    "\n",
    "#Classification report for precision and recall\n",
    "#Random Forest\n",
    "from sklearn.metrics import classification_report\n",
    "Y_preds_RFC = RFC.predict(MinMax_scaled_X_test)\n",
    "print(classification_report(Y_test, Y_preds_RFC))\n",
    "\n",
    "#Decision Trees\n",
    "from sklearn.metrics import classification_report\n",
    "Y_preds_clf = clf.predict(MinMax_scaled_X_test)\n",
    "print(classification_report(Y_test, Y_preds_clf))\n",
    "\n",
    "#K Nearest Neighbours\n",
    "from sklearn.metrics import classification_report\n",
    "Y_preds_neigh = neigh.predict(MinMax_scaled_X_test)\n",
    "print(classification_report(Y_test, Y_preds_neigh))\n",
    "\n",
    "Y_preds_proba_RFC = RFC.predict_proba(MinMax_scaled_X_test)\n",
    "\n",
    "Y_preds_proba_RFC\n",
    "\n",
    "roc_auc_score(Y_test,Y_preds_proba_RFC,multi_class=\"ovr\")\n",
    "\n",
    "Y_preds_proba_DT = DT.predict_proba(MinMax_scaled_X_test)\n",
    "\n",
    "Y_preds_proba_clf\n",
    "\n",
    "roc_auc_score(Y_test,Y_preds_proba_DT,multi_class=\"ovr\")\n",
    "\n",
    "Y_preds_proba_neigh = neigh.predict_proba(MinMax_scaled_X_test)\n",
    "\n",
    "Y_preds_proba_neigh\n",
    "\n",
    "roc_auc_score(Y_test,Y_preds_proba_neigh,multi_class=\"ovr\")\n",
    "\n",
    "# roc curve for classes\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 3\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, Y_preds_proba_RFC[:,i], pos_label=i)\n",
    "    \n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='-.',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='-.',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='-.',color='blue', label='Class 2 vs Rest')\n",
    "#plt.plot(fpr[3], tpr[3], linestyle='-.',color='blue', label='Class 3 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300);    \n",
    "\n",
    "# roc curve for classes\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 3\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, Y_preds_proba_DT[:,i], pos_label=i)\n",
    "    \n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300);    \n",
    "\n",
    "# roc curve for classes\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 3\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, Y_preds_proba_neigh[:,i], pos_label=i)\n",
    "    \n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300);    \n",
    "\n",
    "##########################################\n",
    "#Now checking whether outliers will have impact on the accuracy of the models\n",
    "\n",
    "# check whether there's any significant difference if we do not remove any outliers\n",
    "FILE_NAME = \"data.csv\"\n",
    "df0 = pd.read_csv(FILE_NAME)\n",
    "#still remove all the 0s in gender(as they are illegal entries, also would affect over sample technique)\n",
    "gender_mode = df0.gender.mode() #gender can only be 1,2,3 not 0\n",
    "df0.gender = np.where(df0.gender == 0, gender_mode, df0.gender)\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe.fit_transform(df0[['gender']])\n",
    "\n",
    "target = \"gender\"\n",
    "y0 = df0[target]\n",
    "x0 = df0[['affiliative', 'selfenhancing', 'agressive', 'selfdefeating', 'age','accuracy']]\n",
    "\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(x0, y0, test_size=0.2)\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_smote1_train, Y_smote_train1 = oversample.fit_resample(X_train1.astype('int'),Y_train1)\n",
    "\n",
    "\n",
    "\n",
    "#Here we are going to try a varity of scalers, since min max might be affect by outliers now\n",
    "\n",
    "MinMaxScaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "MinMax_scaled_Xtrain1 = MinMaxScaler.fit_transform(X_smote1_train)\n",
    "MinMax_scaled_Xtest1 = MinMaxScaler.fit_transform(X_test1)\n",
    "#MinMax_scaled_data1 = pd.DataFrame(MinMax_scaled1,columns = X_smote1.columns.values,index = X_smote1.index)\n",
    "#MinMax_scaled_data1\n",
    "\n",
    "np.random.seed(42)\n",
    "RFC1 = RandomForestClassifier()\n",
    "RFC1.fit(MinMax_scaled_Xtrain1,Y_smote_train1.values.ravel())\n",
    "# Check the score of the model (on the test set)\n",
    "print(f\"Random Forest Classifier Accuracy on training set: {RFC1.score(MinMax_scaled_Xtrain1, Y_smote_train1) * 100:.2f}%\")\n",
    "print(f\"Random Forest Classifier Accuracy: {RFC1.score(MinMax_scaled_Xtest1, Y_test1) * 100:.2f}%\")\n",
    "plot_confusion_matrix(RFC1, MinMax_scaled_Xtest1, Y_test1)\n",
    "\n",
    "np.random.seed(42)\n",
    "DT0 = DecisionTreeClassifier(max_depth = 3)\n",
    "DT0.fit(MinMax_scaled_Xtrain1,Y_smote_train1.values.ravel())\n",
    "print(f\"Decision Tree Accuracy on training set: {DT0.score(MinMax_scaled_Xtrain1, Y_smote_train1) * 100:.2f}%\")\n",
    "print(f\"Decision Tree Accuracy: {DT0.score(MinMax_scaled_Xtest1, Y_test1) * 100:.2f}%\")\n",
    "plot_confusion_matrix(DT0, MinMax_scaled_Xtest1, Y_test1)\n",
    "\n",
    "neigh1 = KNeighborsClassifier(n_neighbors= 3)\n",
    "neigh1.fit(MinMax_scaled_Xtrain1,Y_smote_train1.values.ravel())\n",
    "print(f\"K nearest Neightbour Classifier Accuracy on traning set: {neigh1.score(MinMax_scaled_Xtrain1, Y_smote_train1) * 100:.2f}%\")\n",
    "print(f\"K nearest Neightbour Classifier Accuracy: {neigh1.score(MinMax_scaled_Xtest1, Y_test1) * 100:.2f}%\")\n",
    "plot_confusion_matrix(neigh1, MinMax_scaled_Xtest1, Y_test1)\n",
    "\n",
    "#####################\n",
    "#Check whether or not without any up sampling would affect the accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
